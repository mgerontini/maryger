
\section{Conclusions \& Future Work}             % chapter 1
Twitter sentiment analysis has gained much attention during the last years. Many approaches have been implemented in order to produce quality results. In this paper, we tried target dependent sentiment analysis using a combination of methods, while previous works focused only on either machine classification techniques or context analysism without using target dependent queries \cite{jiang2011target}\cite{go2009twitter}.

Moreover, we devised a simple yet effective "PageRank", i.e. authoritativeness, scoring algorithm as an impact factor to assign an additional score based on retweet count, favorite status, number of followers and friends of the author of a tweet. This better models the real world, where it does not matter as much \emph{what} someone says, but whom he says it to.

As mentioned in section \ref{sec:introduction}, there is no absolute measure for sentiment. This makes it difficult to gauge the absolute and relative performance of the algorithm. When using all classifiers and filtering options, an overall precision rate of 71\% is achieved. This may seem barely better than random, but it is quite good when compared to the agreement rates achieved by human classifiers \cite{wilson2005recognizing}. Moreover, it is sufficient to justify absolute claims on reputation: a brand with many more positive than negative tweets will almost certainly have a positive reputation, and vice versa. Thanks to the overabundance of data for some brands, this allows for a more than adequate resolution in both time and sentiment-space for most purposes.

Of course, for relative claims this performance requires more data. Not all tweets can be assigned a sentiment score, so several hundred tweets may be required to reach sufficient levels of confidence. In general, these will be available, but not always. The performance on relative sentiment assessments is thus hampered by the brand with lowest popularity. Comparing \emph{Microsoft} to \emph{Apple} is not a problem, but a comparison with \emph{ChromeOs} must operate on vastly different timescales - a hundred tweets on \emph{Microsoft} are posted within an hour, but the same number of tweets takes five days to accumulate for \emph{ChromeOs}. Moreso, unknown terms are less likely to have been searched for, and are therefore less likely to be stored in the local database. So yes, comparative sentiment analysis can work, but only if all terms are at least somewhat popular.

In the future, it will be important to transform the data using unigrams during the training of the Bayes Classifier instead of using the tokenized, filtered context as a feature vector. In addition to this, an SVM approach for sentiment classification have the potential to be an alternative approach for even better results. Such discriminative methods can include arbitrary features without the restriction to use the conditional independence assumption which is characteristic of the naive Bayes classifier. More preprocessing techniques can affect totally the results and the performance of the sentiment analysis e.g. spelling corrector. The detection of sarcasm and cynicism is another challenge for this topic. Detection of contextually irrelevant content (such as a tweet about Amazon forest, where the search query is Amazon the company) is another difficult improvement area. Social graph structure can be taken into account more for PageRank computation. Moreover, the general tone of an author (i.e. frequently tweeting positively or negatively) can be considered.
